Step 1 is to convert the lichess database (220GB, 100M Games, PGN Format) into a slightly modified version.
We use pgn-extract, a free to use command-line program to manipulate chess games.
Our first version is stripped of all PGN Tags and possible nags. The FEN representation for each move is added as a comment.
Each move and its comments are also printed in a seperate line. 
    pgn-extract -w100000 --fencomments --noresults --notags --nonags --commentlines --quiet --output step1.pgn step0.pgn

The next step is to filter the data using grep and sed. 
We only want boards and games that are evaluated. 
We first group them with grep and only afterwards remove the first line, to additionally get rid of all empty lines.
    grep -B 1 -A 1 --no-group-separator '%eval' step1.pgn | sed '1~3d' > step2.pgn

Afterwards we run a python script (pgn_to_csv.py).
This gets rid of any useless characters and saves each board as a csv format: FEN,eval.

Then we reduce our file size by getting rid of duplicates.
     sort step3.csv | uniq -u > step4.csv
    
By now we successfully reduced our initial size of 220GB to 30GB.
These are still over 440M games. SO no need to worry we run out of traning data.

The lines are sorted. To make the next steps easier we shuffle this file.
Because it is too large for memory we used the rust package rhuffle.
    rhuffle --src step4.csv --dst step5.csv

With the final python script (csv_to_db.py) we convert the csv file into a sqlite database for faster indexing.